## <img src="/Images/Icons/peer_review.png" width="300" height="300" />

## 8. Open Peer Review, Metrics, and Evaluation

### Qu'est-ce que c'est ?

Etre chercheur, c'est se trouver sous évaluation constante. L'académie est une "économie de prestige", où la valeur d'un universitaire repose sur l'évaluation du niveau d'estime que ses pairs, les décideurs et d'autres personnes ont d'eux et de leurs contributions \([Blackmore et Kandiko, 2011](https://doi.org/10/fqrkft)\). Dans cette section, il sera donc utile de faire la distinction entre l'évaluation d'un travail et l'évaluation du chercheur lui-même. La recherche et les chercheurs sont évalués à l'aide de deux méthodes principales : l'évaluation par les pairs et les paramètres, le premier qualitatif et le second quantitatif. 

L'évaluation par les pairs sert principalement à juger des éléments de recherche. Il s'agit du mécanisme officiel d'assurance de la qualité par lequel les manuscrits savants (p. ex. articles de journaux, livres, demandes de subventions et communications à des conférences) sont soumis à l'examen minutieux d'autres personnes, dont les commentaires et les jugements sont ensuite utilisés pour améliorer les travaux et prendre des décisions finales concernant la sélection (pour publication, attribution de subventions ou temps de parole). L'évaluation par les pairs ouverte signifie différentes choses pour différentes personnes et communautés et a été défini comme "un terme générique pour un certain nombre de façons dont les modèles d'évaluation par les pairs peuvent être adaptés conformément aux objectifs de la science ouverte " \([Ross-Hellauer, 2017](https://doi.org/10/gc5sjh)\). Ses deux principales caractéristiques sont les "identités ouvertes", où les auteurs et les examinateurs sont tous deux conscients de l'identité de l'autre \(c.-à-d. non aveugles\), et les "rapports ouverts", où les rapports d'évaluation sont publiés avec l'article pertinent. Ces caractéristiques peuvent être combinées, mais ne doivent pas nécessairement l'être, et peuvent être complétées par d'autres innovations, telles que la "participation ouverte", où les membres de la communauté au sens large peuvent contribuer au processus d'examen, l'"interaction ouverte", où la discussion réciproque directe entre auteur·rice·s et examinateur·rice·s, et/ou entre examinateur·rice·s, est autorisée et encouragée, et les "manuscrits ouverts avant évaluation", où les manuscrits sont rendus immédiatement disponibles avant toute procédure officielle d'évaluation par les pairs \(soit dans le cadre du travail des revues ou en dehors par des serveurs de pré-impression\). 

Une fois qu'elles ont passé l'évaluation par les pairs, les publications de recherche sont souvent la principale mesure du travail d'un chercheur (d'où l'expression "publier ou périr"). Cependant, l'évaluation de la qualité des publications est difficile et subjective. Bien que certains exercices d'évaluation générale comme le Research Excellence Framework du Royaume-Uni utilisent l'évaluation par les pairs, l'évaluation générale est souvent basée sur des __metriques__ comme le nombre de citations que les publications recueillent \(indice h\), ou même le niveau perçu de prestige de la revue dans laquelle elles ont été publiées \(quantifié par le Journal Impact Factor\). La prédominance de ces mesures et la manière dont elles peuvent fausser les incitations ont été soulignées ces dernières années dans des déclarations comme le [Manifeste de Leiden](http://www.leidenmanifesto.org/) et la [Déclaration de San Francisco sur l'évaluation de la recherche (DORA)](https://sfdora.org/).

Au cours des dernières années, "Alternative Metrics" ou [altmetrics](https://www.altmetric.com) sont devenus un sujet dans le débat sur une évaluation équilibrée des efforts de recherche qui complètent le comptage des citations en incorporant d'autres mesures en ligne de l'impact de la recherche, notamment les signets, liens, billets de blog, tweets, likes, actions, couverture presse et autres. La base de tous ces problèmes avec les métriques est qu'elles sont beaucoup produites par des entités commerciales (par exemple, Clarivate Analytics et Elsevier) basées sur des systèmes propriétaires, ce qui peut conduire à des problèmes de transparence.

## <img src="/Images/Icons/umbrella.png" width="150" height="150" />
### Justification

#### Évaluation par les pairs ouverte
À partir du XVIIe siècle, avec la Royal Society of London (1662) et l'Académie des Sciences de Paris (1699), en tant que privilège de la science de s'autocensurer plutôt que de passer par l'église, il a fallu de nombreuses années pour que l'évaluation par les pairs soit correctement établi dans la science. L'évaluation par les pairs, en tant que mécanisme officiel, est beaucoup plus jeune que beaucoup le supposent. Par exemple, la revue Nature ne l'a introduite qu'en 1967. Bien que les sondages montrent que les chercheurs accordent de l'importance à l'évaluation par les pairs, ils pensent aussi qu'elle pourrait mieux fonctionner. On se plaint souvent que l'évaluation par les pairs prend trop de temps, qu'elle est incohérente, qu'elle ne détecte pas souvent les erreurs et que l'anonymat protège les préjugés. L'évaluation par les pairs ouverte vise donc à accroître la transparence et la participation aux processus formels et informels d'évaluation par les pairs. Le rôle de relecteur·rice offre aux chercheurs l'occasion de participer à de nouvelles recherches, d'établir des réseaux universitaires et d'acquérir de l'expertise, et de perfectionner leurs propres compétences en rédaction. Il s'agit d'un élément crucial du contrôle de la qualité des travaux universitaires. Pourtant, en général, les chercheurs ne reçoivent pas souvent une formation officielle sur la façon de procéder à l'évaluation. Toutefois, même lorsque les chercheurs se croient en confiance avec l'évaluation par les pairs traditionnelle, les nombreuses formes d'évaluation ouverte présentent de nouveaux défis et de nouvelles possibilités. Comme l'évaluation par les pairs ouverte couvre un éventail aussi diversifié de pratiques, les examinateur·rice·s et les auteur·rice·s doivent tenir compte de nombreux facteurs. 

## <img src="/Images/02 Open Science Basics/02_open_peer_review.png" />

En ce qui concerne l'évaluation, les récompenses et les mesures actuelles en science et en érudition ne sont pas (encore) conformes à l'Open Science. Les paramètres utilisés pour évaluer la recherche \(p. ex. le Journal Impact Factor, l'indice h\) ne mesurent pas - et ne récompensent donc pas - les pratiques de recherche ouvertes. L'activité d'évaluation par les pairs ouverte n'est pas nécessairement reconnue  dans les scénarios d'avancement professionnel \(par exemple, dans de nombreux cas, les évaluateurs de subventions ne considèrent pas même les évaluations les plus brillantes comme des objets savants en soi\). De plus, de nombreux paramètres d'évaluation - en particulier certains types de bibliométrie - ne sont pas aussi ouverts et transparents que la communauté le souhaiterait.

Dans ces circonstances, au mieux, les pratiques de science ouverte sont considérées comme un fardeau supplémentaire sans récompense. Dans le pire des cas, ils sont perçus comme nuisant activement aux chances de financement et de promotion futurs ainsi qu'à la pérénnisation. Un récent [rapport de la Commission européenne (2017)](https://doi.org/10.2777/75255) reconnaît qu'il existe essentiellement deux approches à la mise en œuvre de la science ouverte et la façon dont les récompenses et l'évaluation peuvent soutenir cela :

1. Il suffit d'appuyer le statu quo en encourageant une plus grande ouverture, en établissant des mesures connexes et en quantifiant les résultats ;

2. Expérimenter des pratiques de recherche et d'évaluation alternatives, des données ouvertes, la science citoyenne et l'éducation ouverte.

De plus en plus de bailleurs de fonds et d'institutions prennent des mesures dans ce sens, par exemple en s'éloignant des simples comptages et en incluant dans leurs exercices d'évaluation des récits et des indications sur l'impact sociétal. D'autres mesures prises par les organismes financeurs permettent un plus grand nombre de types de résultats de recherche \(comme les pré-impressions\) dans les demandes et financent différents types de recherche \(comme les études de réplication\).

## <img src="/Images/Icons/finish.png" width="150" height="150" />
### Objectifs d'apprentissage

1. Reconnaître les éléments clés de l'évaluation ouverte et leurs avantages et inconvénients potentiels.
2. Comprendre les différences entre les types de mesures utilisées pour évaluer la recherche et les chercheurs.
3. S'engager dans le débat sur la façon dont le schéma d'évaluation influe sur la façon dont la recherche est effectuée.

### Composantes clés
## <img src="/Images/Icons/brain.png" width="150" height="150" />
### Connaissances
#### L'évaluation ouverte

Parmi les lieux de diffusion les plus populaires de l'évaluation ouverte figurent les revues d'éditeurs tels que Copernicus, Frontiers, BioMed Central, eLife et F1000research.

L'évaluation ouverte, sous ses différentes formes, présente de nombreux avantages potentiels pour les évaluateurs et les auteurs :

* Les identités ouvertes (non aveugles) favorisent une plus grande responsabilisation parmi les examinateurs et réduisent les possibilités de partialité ou de conflits d'intérêts non divulgués.

* Les rapports d'évaluation ouverts ajoutent un autre niveau d'assurance de la qualité, permettant à la communauté élargie d'examiner les évaluations pour déterminer les processus décisionnels.

* En combinaison, les identités ouvertes et les rapports ouverts sont théorisés pour mener à de meilleures évaluations, car l'idée que leur nom soit publiquement lié à une œuvre ou que leur évaluation soit publiée encourage les examinateurs à être plus rigoureux.

* Les identités ouvertes et les rapports ouverts permettent aux examinateurs d'obtenir le crédit du public pour leur travail de relecture, ce qui encourage cette activité vitale et permet de citer le travail d'évaluation dans d'autres publications et dans les activités de développement de carrière liées à la promotion et à la pérennisation.

* Une participation ouverte pourrait permettre de surmonter les problèmes associés à la sélection éditoriale des examinateurs (p. ex. préjugés, réseaux fermés, élitisme). Surtout pour les chercheurs en début de carrière qui ne reçoivent pas encore d'invitation à l'évaluation, de tels processus ouverts peuvent aussi leur donner l'occasion d'établir leur réputation en recherche et de mettre en pratique leurs compétences en évaluation.

Il y a des pièges potentiels à éviter, notamment :

* Les identités ouvertes suppriment les conditions d'anonymat pour les évaluateurs \(simple-aveugle\) ou les auteurs et évaluateurs \(double-aveugle\) qui sont traditionnellement en place pour contrer les préjugés sociaux \(bien que l'efficacité d'un tel anonymat ne soit pas démontrée avec force\). Il est donc important que les examinateurs remettent constamment en question leurs hypothèses pour s'assurer que leurs jugements ne reflètent que la qualité du manuscrit, et non le statut, l'historique ou les affiliations de l'auteur\(s\). Les auteurs devraient faire de même lorsqu'ils reçoivent des commentaires d'évaluation par les pairs.

* Donner et recevoir des critiques est souvent un processus chargé de réactions émotionnelles inévitables - les auteurs et les réviseurs peuvent subjectivement être d'accord ou non sur la façon de présenter les résultats et/ou sur ce qui doit être amélioré, modifié ou corrigé. Dans les identités ouvertes et/ou les rapports ouverts, la transparence pourrait exacerber ces difficultés. Il est donc essentiel que les examinateurs s'assurent qu'ils communiquent leurs arguments de manière claire et courtoise, afin de maximiser les chances qu'ils soient reçus sous forme de commentaires précieux de la part de l'auteur.

* Le manque d'anonymat des examinateurs dans le cadre d'une évaluation ouverte pourrait renverser le processus en décourageant les examinateurs de formuler des critiques sévères, surtout à l'endroit de leurs collègues de rang supérieur.

* Enfin, compte tenu de ces questions, les examinateurs potentiels pourraient être plus susceptibles de refuser de procéder à l'évaluation.

#### Les indicateurs de performances ouverts

La [Déclaration de San Francisco sur l'évaluation de la recherche \(DORA\)](https://sfdora.org/) recommande de s'éloigner des évaluations fondées sur des revues, d'examiner tous les types de résultats et d'utiliser diverses formes d'indicateurs de performances et d'évaluation de la narration en parallèle. DORA a été signé par des milliers de chercheurs, d'institutions, d'éditeurs et de bailleurs de fonds, qui se sont maintenant engagés à le mettre en pratique. Le [Manifeste de Leyde](http://www.leidenmanifesto.org/) fournit des conseils sur la façon d'utiliser les mesures de manière responsable.

En ce qui concerne l'altimétrie, [Priem et al. \(2010\)](http://altmetrics.org/manifesto/) indiquent que ces paramètres présentent les avantages suivants : ils s'accumulent plus rapidement que les citations ; ils permettent de mesurer l'impact des résultats de recherche autres que les publications (p. ex. ensembles de données, codes, protocoles, blogues, tweets, etc.) ; et elle peut fournir diverses mesures des effets pour chaque objet. L'actualité de l'altimétrie présente un avantage particulier pour les chercheurs en début de carrière, dont l'impact sur la recherche peut ne pas encore se traduire par un nombre important de citations, mais dont la progression de carrière dépend d'évaluations positives. De plus, l'altimétrie peut aider à identifier rapidement les recherches influentes et les liens potentiels entre les chercheurs. Un rapport récent du groupe d'experts de la CE sur l'altmétrie ([Wilsdon et al. \(Commission européenne\), 2017](https://ec.europa.eu/research/openscience/pdf/report.pdf)) a identifié les défis de l'altmétrie, y compris le manque de robustesse et de sensibilité au "jeu"; que toute mesure cesse d'être une bonne mesure lorsqu'elle devient une cible \("loi Goodhart"\); le manque relatif d'adoption des médias sociaux dans certaines disciplines et régions géographiques; et une dépendance sur les entités commerciales pour les données de base.

## <img src="/Images/Icons/gears.png" width="150" height="150" />
### Savoir-faire

Exemples d'exercices

* Les personnels en formation travaillent par groupes de trois. Chacun d'eux rédige individuellement une évaluation d'un court texte académique.

* Révision d'un article sur un serveur de pré-impression

* Utiliser un service gratuit de bibliométrie ou d'altmétrie \(par exemple [Impactstory](https://impactstory.org/), [Paperbuzz](https://paperbuzz.org/), [Altmetric bookmarklet](https://www.altmetric.com/products/free-tools/bookmarklet/), [Dimensions.ai](https://www.dimensions.ai/)\) pour rechercher des mesures pour un article, puis écrivez une brève explication sur la façon dont sont calculés exactement différents paramètres déclarés par chaque service \(c'est plus difficile qu'on ne le pense, car cela implique de trouver la documentation appropriée des paramètres, y compris les services apparemment plus transparents\).

## <img src="/Images/Icons/questions.png" width="150" height="150" height="150" />
### Questions, obstacles et idées fausses courantes
Q : L'évaluation de la recherche est-elle équitable ?

R : L'évaluation de la recherche est aussi équitable que ses méthodes et ses techniques d'évaluation. Les mesures et l'altimétrie tentent de mesurer la qualité de la recherche à l'aide de la quantité de résultats de la recherche, qui peut être exacte, mais qui ne doit pas nécessairement l'être.

## <img src="/Images/Icons/output.png" width="150" height="150" />
### Résultats d'apprentissage

1. Les personnels en formation seront en mesure d'identifier les revues avec évaluation ouverte par les pairs.
2. Les personnels en formation seront au courant d'une série d'indicateurs de performances, de leurs avantages et de leurs inconvénients.

## <img src="/Images/Icons/magnifying_glass.png" width="150" height="150" />
### Pour en savoir plus

* Directorate-General for Research and Innovation (European Commission) (2017). Evaluation of Research Careers Fully Acknowledging Open Science Practices: Rewards, Incentives and/or Recognition for Researchers Practicing Open Science. [doi.org/10.2777/75255](https://doi.org/10.2777/75255)

* Hicks et al. (2015) Bibliometrics: The Leiden Manifesto for research metrics. [doi.org/10.1038/520429a](www.doi.org/10.1038/520429a), [leidenmanifesto.org](http://www.leidenmanifesto.org/)

* Peer Review the Nuts and Bolts (2012). A Guide for Early Career Researchers. [PDF](http://senseaboutscience.org/wp-content/uploads/2016/09/peer-review-the-nuts-and-bolts.pdf)


### Projets et initiatives

* Make Data Count. [makedatacount.org](https://makedatacount.org/)

* NISO Alternative Assessment Metrics \(Altmetrics\) Initiative. [niso.org](http://www.niso.org/standards-committees/altmetrics)

* Open Rev. [openrev.org](https://en.wikipedia.org/wiki/Open_Rev)

* OpenUP Hub. [openuphub.eu](https://www.openuphub.eu/review)

* Peer Reviewers’ Openness Initiative. [opennessinitiative.org](https://opennessinitiative.org/)

* Peerage of Science. A free service for scientific peer review and publishing. [peerageofscience.org](https://www.peerageofscience.org/)

* Responsible Metrics. [responsiblemetrics.org](https://responsiblemetrics.org/)

* Snowball Metrics. Standardized research metrics - by the sector for the sector. [snowballmetrics.com](https://www.snowballmetrics.com/)
