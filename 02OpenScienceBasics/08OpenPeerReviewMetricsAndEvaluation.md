## <img src="/Images/Icons/peer_review.png" width="300" height="300" />

## 8. Open Peer Review, Metrics, and Evaluation

### Qu'est-ce que c'est ?

Etre chercheur, c'est se trouver sous évaluation constante. L'académie est une "économie de prestige", où la valeur d'un universitaire repose sur l'évaluation du niveau d'estime que ses pairs, les décideurs et d'autres personnes ont d'eux et de leurs contributions \([Blackmore et Kandiko, 2011](https://doi.org/10/fqrkft)\). Dans cette section, il sera donc utile de faire la distinction entre l'évaluation d'un travail et l'évaluation du chercheur lui-même. La recherche et les chercheurs sont évalués à l'aide de deux méthodes principales : l'évaluation par les pairs et les paramètres, le premier qualitatif et le second quantitatif. 

L'évaluation par les pairs sert principalement à juger des éléments de recherche. Il s'agit du mécanisme officiel d'assurance de la qualité par lequel les manuscrits savants (p. ex. articles de journaux, livres, demandes de subventions et communications à des conférences) sont soumis à l'examen minutieux d'autres personnes, dont les commentaires et les jugements sont ensuite utilisés pour améliorer les travaux et prendre des décisions finales concernant la sélection (pour publication, attribution de subventions ou temps de parole). L'évaluation par les pairs ouverte signifie différentes choses pour différentes personnes et communautés et a été défini comme "un terme générique pour un certain nombre de façons dont les modèles d'évaluation par les pairs peuvent être adaptés conformément aux objectifs de la science ouverte " \([Ross-Hellauer, 2017](https://doi.org/10/gc5sjh)\). Ses deux principales caractéristiques sont les "identités ouvertes", où les auteurs et les examinateurs sont tous deux conscients de l'identité de l'autre \(c.-à-d. non aveugles\), et les "rapports ouverts", où les rapports d'évaluation sont publiés avec l'article pertinent. Ces caractéristiques peuvent être combinées, mais ne doivent pas nécessairement l'être, et peuvent être complétées par d'autres innovations, telles que la "participation ouverte", où les membres de la communauté au sens large peuvent contribuer au processus d'examen, l'"interaction ouverte", où la discussion réciproque directe entre auteur·rice·s et examinateur·rice·s, et/ou entre examinateur·rice·s, est autorisée et encouragée, et les "manuscrits ouverts avant évaluation", où les manuscrits sont rendus immédiatement disponibles avant toute procédure officielle d'évaluation par les pairs \(soit dans le cadre du travail des revues ou en dehors par des serveurs de pré-impression\). 

Une fois qu'elles ont passé l'évaluation par les pairs, les publications de recherche sont souvent la principale mesure du travail d'un chercheur (d'où l'expression "publier ou périr"). Cependant, l'évaluation de la qualité des publications est difficile et subjective. Bien que certains exercices d'évaluation générale comme le Research Excellence Framework du Royaume-Uni utilisent l'évaluation par les pairs, l'évaluation générale est souvent basée sur des __metriques__ comme le nombre de citations que les publications recueillent \(indice h\), ou même le niveau perçu de prestige de la revue dans laquelle elles ont été publiées \(quantifié par le Journal Impact Factor\). La prédominance de ces mesures et la manière dont elles peuvent fausser les incitations ont été soulignées ces dernières années dans des déclarations comme le [Manifeste de Leiden](http://www.leidenmanifesto.org/) et la [Déclaration de San Francisco sur l'évaluation de la recherche (DORA)](https://sfdora.org/).

Au cours des dernières années, "Alternative Metrics" ou [altmetrics](https://www.altmetric.com) sont devenus un sujet dans le débat sur une évaluation équilibrée des efforts de recherche qui complètent le comptage des citations en incorporant d'autres mesures en ligne de l'impact de la recherche, notamment les signets, liens, billets de blog, tweets, likes, actions, couverture presse et autres. La base de tous ces problèmes avec les métriques est qu'elles sont beaucoup produites par des entités commerciales (par exemple, Clarivate Analytics et Elsevier) basées sur des systèmes propriétaires, ce qui peut conduire à des problèmes de transparence.

## <img src="/Images/Icons/umbrella.png" width="150" height="150" />
### Justification

#### Évaluation par les pairs ouverte
À partir du XVIIe siècle, avec la Royal Society of London (1662) et l'Académie des Sciences de Paris (1699), en tant que privilège de la science de s'autocensurer plutôt que de passer par l'église, il a fallu de nombreuses années pour que l'évaluation par les pairs soit correctement établi dans la science. L'évaluation par les pairs, en tant que mécanisme officiel, est beaucoup plus jeune que beaucoup le supposent. Par exemple, la revue Nature ne l'a introduite qu'en 1967. Bien que les sondages montrent que les chercheurs accordent de l'importance à l'évaluation par les pairs, ils pensent aussi qu'elle pourrait mieux fonctionner. On se plaint souvent que l'évaluation par les pairs prend trop de temps, qu'elle est incohérente, qu'elle ne détecte pas souvent les erreurs et que l'anonymat protège les préjugés. L'évaluation par les pairs ouverte vise donc à accroître la transparence et la participation aux processus formels et informels d'évaluation par les pairs. Le rôle de relecteur·rice offre aux chercheurs l'occasion de participer à de nouvelles recherches, d'établir des réseaux universitaires et d'acquérir de l'expertise, et de perfectionner leurs propres compétences en rédaction. Il s'agit d'un élément crucial du contrôle de la qualité des travaux universitaires. Pourtant, en général, les chercheurs ne reçoivent pas souvent une formation officielle sur la façon de procéder à l'évaluation. Toutefois, même lorsque les chercheurs se croient en confiance avec l'évaluation par les pairs traditionnelle, les nombreuses formes d'évaluation ouverte présentent de nouveaux défis et de nouvelles possibilités. Comme l'évaluation par les pairs ouverte couvre un éventail aussi diversifié de pratiques, les examinateur·rice·s et les auteur·rice·s doivent tenir compte de nombreux facteurs. 

## <img src="/Images/02 Open Science Basics/02_open_peer_review.png" />

Regarding evaluation, current rewards and metrics in science and scholarship are not \(yet\) in line with Open Science. The metrics used to evaluate research \(e.g. Journal Impact Factor, h-index\) do not measure - and therefore do not reward - open research practices. Open peer review activity is not necessarily recognized as "scholarship" in professional advancement scenarios \(e.g. in many cases, grant reviewers don’t consider even the most brilliant open peer reviews to be scholarly objects unto themselves\). Furthermore, many evaluation metrics - especially certain types of bibliometrics - are not as open and transparent as the community would like.

Under those circumstances, at best Open Science practices are seen as an additional burden without rewards. At worst, they are seen as actively damaging chances of future funding and promotion as well as tenure. A recent [report from the European Commission (2017)](https://doi.org/10.2777/75255) recognizes that there are basically two approaches to Open Science implementation and the way rewards and evaluation can support that:

1. Simply support the status quo by encouraging more openness, building related metrics and quantifying outputs;

2. Experiment with alternative research practices and assessment, open data, citizen science and open education.

More and more funders and institutions are taking steps in these directions, for example by moving away from simple counts, and including narratives and indications of societal impact in their assessment exercises. Other steps funders are taking are allowing more types of research output \(such as preprints\) in applications and funding different types of research \(such as replication studies\).

## <img src="/Images/Icons/finish.png" width="150" height="150" />
### Learning objectives

1. Recognise the key elements of open peer review and their potential advantages and disadvantages
2. Understand the differences between types of metrics used to assess research and researchers
3. Engage with the debate over the way in which evaluation schema affect the ways in which scholarship is performed

### Key components
## <img src="/Images/Icons/brain.png" width="150" height="150" />
### Knowledge
#### Open peer review

Popular venues for OPR include journals from publishers like Copernicus, Frontiers, BioMed Central, eLife and F1000research.

Open peer review, in its different forms, has many potential advantages for reviewers and authors:

* Open identities \(non-blinded\) review fosters greater accountability amongst reviewers and reduces the opportunities for bias or undisclosed conflicts of interest.

* Open review reports add another layer of quality assurance, allowing the wider community to scrutinize reviews to examine decision-making processes.

* In combination, open identities and open reports are theorized to lead to better reviews, as the thought of having their name publicly connected to a work or seeing their review published encourages reviewers to be more thorough.

* Open identities and open reports enable reviewers to gain public credit for their review work, thus incentivising this vital activity and allowing review work to be cited in other publications and in career development activities linked to promotion and tenure.

* Open participation could overcome problems associated with editorial selection of reviewers \(e.g., biases, closed-networks, elitism\). Especially for early career researchers who do not yet receive invitations to review, such open processes may also present a chance to build their research reputation and practice their review skills.

There are some potential pitfalls to watch out for, including:

* Open identities removes anonymity conditions for reviewers \(single-blind\) or authors and reviewers \(double-blind\) which are traditionally in place to counteract social biases \(although there is not strong-evidence that such anonymity has been effective\). It’s therefore important for reviewers to constantly question their assumptions to ensure their judgements reflect only the quality of the manuscript, and not the status, history, or affiliations of the author\(s\). Authors should do the same in receiving peer review comments.

* Giving and receiving criticism is often a process fraught with unavoidably emotional reactions - authors and reviewers may subjectively agree or disagree on how to present the results and/or what needs improvement, amendment or correction. In open identities and/or open reports, the transparency could exacerbate such difficulties. It is therefore essential that reviewers ensure that they communicate their points in a clear and civil way, in order to maximise the chances that it will be received as valuable feedback by the author\(s\).

* Lack of anonymity for reviewers in open identities review might subvert the process by discouraging reviewers from making strong criticisms, especially against higher-status colleagues.

* Finally, given these issues, potential reviewers may be more likely to decline to review.

#### Open metrics

The [San Francisco Declaration on Research Assessment \(DORA\)](https://sfdora.org/) recommends moving away from journal based evaluations, consider all types of output and use various forms of metrics and narrative assessment in parallel. DORA has been signed by thousands of researchers, institutions, publishers and funders, who have now committed themselves to putting this in practice. The [Leiden Manifesto](http://www.leidenmanifesto.org/) provides guidance on how to use metrics responsibly.

Regarding Altmetrics, [Priem et al. (2010)](http://altmetrics.org/manifesto/) advise that altmetrics have the following benefits: they accumulate quicker than citations; they can gauge the impact of research outputs other than journal publications (e.g. datasets, code, protocols, blog posts, tweets, etc.); and they can provide diverse measures of impact for individual objects. The timeliness of altmetrics presents a particular advantage to early-career researchers, whose research-impact may not yet be reflected in significant numbers of citations, yet whose career-progression depends upon positive evaluations. In addition, altmetrics can help with early identification of influential research and potential connections between researchers. A recent report by the EC’s Expert Group on Altmetrics ([Wilsdon et al. (European Commission), 2017](https://ec.europa.eu/research/openscience/pdf/report.pdf)) identified challenges of altmetrics, including lack of robustness and susceptibility to ‘gaming’; that any measure ceases to be a good measure once it becomes a target (‘Goodhart’s Law’); relative lack of social media uptake in some disciplines and geographical regions; and a reliance on commercial entities for the underlying data.

## <img src="/Images/Icons/gears.png" width="150" height="150" />
### Skills

Example exercises

* Trainees work in groups of three. Each individually writes a review of a short academic text

* Review a paper on a pre-print server

* Use a free bibliometrics or altmetrics service \(e.g. [Impactstory](https://impactstory.org/), [Paperbuzz](https://paperbuzz.org/), [Altmetric bookmarklet](https://www.altmetric.com/products/free-tools/bookmarklet/), [Dimensions.ai](https://www.dimensions.ai/)\) to look up metrics for a paper, then write a short explanation of how exactly various metrics reported by each service are calculated \(it’s harder than you’d assume; this would get at the challenges of finding proper metrics documentation for even the seemingly most transparent services\)

## <img src="/Images/Icons/questions.png" width="150" height="150" />
### Questions, obstacles, and common misconceptions
Q: Is research evaluation fair?

A: Research evaluation is as fair as its methods and evaluation techniques. Metrics and altmetrics try to measure research quality with research output quantity, which can be accurate, but does not have to be.


## <img src="/Images/Icons/output.png" width="150" height="150" />
### Learning outcomes

1. Trainees will be able to identify open peer review journals
2. Trainees will be aware of a range of metrics, their advantages and disadvantages

## <img src="/Images/Icons/magnifying_glass.png" width="150" height="150" />
### Further reading

* Directorate-General for Research and Innovation (European Commission) (2017). Evaluation of Research Careers Fully Acknowledging Open Science Practices: Rewards, Incentives and/or Recognition for Researchers Practicing Open Science. [doi.org/10.2777/75255](https://doi.org/10.2777/75255)

* Hicks et al. (2015) Bibliometrics: The Leiden Manifesto for research metrics. [doi.org/10.1038/520429a](www.doi.org/10.1038/520429a), [leidenmanifesto.org](http://www.leidenmanifesto.org/)

* Peer Review the Nuts and Bolts (2012). A Guide for Early Career Researchers. [PDF](http://senseaboutscience.org/wp-content/uploads/2016/09/peer-review-the-nuts-and-bolts.pdf)


### Projects and initiatives

* Make Data Count. [makedatacount.org](https://makedatacount.org/)

* NISO Alternative Assessment Metrics \(Altmetrics\) Initiative. [niso.org](http://www.niso.org/standards-committees/altmetrics)

* Open Rev. [openrev.org](https://en.wikipedia.org/wiki/Open_Rev)

* OpenUP Hub. [openuphub.eu](https://www.openuphub.eu/review)

* Peer Reviewers’ Openness Initiative. [opennessinitiative.org](https://opennessinitiative.org/)

* Peerage of Science. A free service for scientific peer review and publishing. [peerageofscience.org](https://www.peerageofscience.org/)

* Responsible Metrics. [responsiblemetrics.org](https://responsiblemetrics.org/)

* Snowball Metrics. Standardized research metrics - by the sector for the sector. [snowballmetrics.com](https://www.snowballmetrics.com/)
